엘라스틱 서치가 제공하는 다양한 API를 다룰 수 있는 챕터

## 1.단건 문서 API

하나의 문서에 대한 조작을 진행할 수 있는 API이다.

### 색인 API

- 앞 장에서 이야기했던 API들이다. POST, PUT, id값을 추가하거나 추가하지 않아서 문서를 만들 수 있다.
- 성능 향상을 위해 라우팅 기능을 추가하여 색인을 진행할 수도 있다.
- **refresh**라는 매개변수를 통해 색인한 직후 검색할 수 있게도 할 수 있다.
  - 예상하겠지만 비용이 크게 들기에 많은 고려가 필요하다.
  
### 조회 API

- `GET`을 이용한 API들이다. _doc,_source로 조회가 가능하다.
  - _doc은 전체 메타데이터를 확인할 수 있지만, _source는 메타데이터 없이 본문만 조회 가능하다.
- 검색과 다르게 색인이 refresh되지 않은 상태에서도 변경된 내용을 확인할 수 있다.
- **필드 필터링**을 통해서 결과에 원하는 필드만 필터링해 포함시킬 수 있다.
  - _soruce_includes: 지정한 필드만을 결과에 포함한다.
  - _source_excludes: 지정한 필드외에 값을 결과에 포함한다.
  - `GET myindex2/_doc/1?_source_includes=t*` 이런식으로 와일드카드로 표현도 가능하다.
- 색인했을 때 라우팅을 지정하였다면 조회시에도 라우팅키값으로 조회해야한다.

### 업데이트 API

- PUT으로 색인이 생성가능하므로 일반적인 API와 달리 POST + _update라는 구문으로 업데이트할 수 있다.
- **업데이트 방식은 기존 문서를 수정하는게 아닌, 기존 문서와 업데이트 될 내용을 합쳐서 새 문서로 만들어 색인한다.**
- doc에 내용을 **직접 입력**하여 업데이트 할 수도 있다.
- **detect_noop**이란 업데이트 내용이 기존 문서 내용을 실질적으로 변경하는 여부를 확인하여 만약 기존 문서와 동일하면 noop 요청이라고 판단하고 쓰기 작업을 진행하지 않는다. 기본적으로 활성화되어 있다.
- 업데이트 API는 기존 문서를 읽는 동작방식을 지닌다 했는데, 만약 기존 문서가 없다면 **doc_as_upsert**라는 옵션을 통해서 새로 문서를 추가할 지 여부를 추가할 수 있다.
- script를 통한 업데이트도 가능한데, 엘라스틱서치의 자체 스크립트 언어인 painless를 사용한다. 더 궁금하면 찾아보면 될 듯 하다.
- 색인 API와 마찬가지로 라우팅과 refresh 옵션을 지정할 수 있다.


### 삭제 API

지정한 문서 하나를 삭제한다. `DELETE` 메서드를 사용하면 되고, 큰 특이사항은 없다. 조심해야할건 `DELETE [인덱스 이름]` 으로 개발자가 실수하여 doc를 지정하지 않으면 인덱스 전체가 삭제될 수 있다는 것이다.

## 2. 복수 문서 API

단건 API를 여러번 날리는건 네트워크 비용이 들 수 밖에 없다.

한 번의 요청으로 여러 요구사항을 처리할 수 있으면 좋다. 그래서 복수 문서에 대한 제어가 필요하면 여기서 설명되어지는 API를 사용하면 될 것 같다.

### bulk API

여러 색인, 업데이트 삭제 작업을 한 번의 요청에 담아서 보내는 API다.

- NDJSON이라는 여러 줄의 JSON을 줄바꿈 문자로 구분하여 요청을 보내는 형태를 사용한다. 따라서 헤더도 application/json 대신 application/x-ndjson을 사용하고, 가장 마지막 줄 줄바꿈 문자 \n으로 끝나야 한다.

```json
POST _bulk
{"index": {"_index": "bulk_test","id":"1"}}
...
```

이런식으로 사용이 가능하다.

- 색인 요청을 할 때 index, create 요청이 사용되는데, index는 기존에 동일한 _id가 존재하면 덮어 씌우고 create는 새 문서를 생성하는 것만 허용한다.
- `POST [인덱스 이름]/_bulk` 이런식으로 인덱스 이름을 넣으면 요청의 기본 대상이 해당 인덱스로 지정된다.
- **여러 요청에 대한 작업 순서는 랜덤이다.** 그 이유는 노드가 요청을 수신하면 각 요청의 내용을 보고 적절한 주 샤드로 요청을 넘기는 작업이 있기 때문이다. 단, 이러한 동작으로 인해 동일한 문서에 대한 bulk작업은 순서 보장이 된다.


### multi get API


_id를 여럿 지정하여 해당 문서를 한 번에 조회하는 API

- `GET _mget` 요청을 통해 수행할 수 있으며 본문에 `docs` 키에 조회할 문서들을 담으면 된다. 이러면 응답은 여러 문서에 대한 내용이 한 번에 오게 된다.

### update by query, delete by query

검색 쿼리를 통해 주어진 조건을 만족하는 문서를 찾은 뒤 그 문서를 대상으로 업데이트나 삭제 작업을 실시하는 API다.

- `POST [인덱스 이름]/_updated_by_query` 
- 업데이트 도중 다른 동작으로 인해 문서에 변화가 생길 수 있는데, 특정 옵션을 통해 이런 충돌에 대한 대응을 할 수 있다. 기본값은 abort로 충돌 발견 시 작업을 중단한다. 다른 상태로는 proceed가 있다. **참고로 충돌이 발생하고 작업이 멈추면 롤백되지 않아서 중단 전까지 변경된 내용은 그대로 남아 있는다.**
- **스로틀링 기능**이 있는데, 이 기능을 통해 작업의 속도를 조정하고 클러스터 부하와 서비스 영향을 최소화할 수 있다. 스크롤링 사이즈를 지정하면 그 사이즈만큼 작업을 순차로 진행한다. 예로 들어서 스크롤링 사이즈를 1000으로 하였다면 1~1000개의 문서를 가져온 뒤 업데이트를 수행하고 1001~2000번의 문서를 가져와서 작업을 수행한다.
- scroll_size를 통해 한 번에 가져올 문서 크기를 정할 수 있고, scroll=1m와 같은 값을 통해 가져온 문서에 대한 데이터들을 얼머나 보존할 지 정할 수 있다. 모든 작업이 종료될 때까지 필요한 시간을 지정하는 것이 아닌, 한 배치 작업에 필요한 시간을 지정하면 된다.
- 스로틀링을 실제 적용하기 위해서는 `requests_per_second` 설정을 이용한다. 이 값은 이름 그대로 평균적으로 초당 몇 개까지의 작업을 수행할 것인지를 지정한다. 예를 들어서 `scroll_size`가 2000이고, `requests_per_second`값이 500이라면 다음과 같이 동작할 수 있다.
  - 만약 2000개의 문서를 업데이트하는데 0.5초를 사용했다면 3.5초를 기다려서 초당 500개의 문서를 처리한 것처럼 대기시간을 조절하여 4초마다 스크롤 하나의 분량을 처리할 수 있게 한다(requests_per_second가 초당 500개의 분량을 해결할 수 있도록 설정하였으므로)
- `wait_for_completion` 매개변수를 통해 update by query에 대한 비동기 요청이 가능하다.
  - update by query에 대한 동작은 task의 형태로 동작하며 해당 값에 부여된id를 통하여 진행상태 조회가 가능하다. 비동기로 동작시키게끔 하면 .task라는 내부 인덱스에 문서로 저장되고 여기서 진행 상황을 확인할 수 있다.(`GET _task/[task id]`)
    - `POST _tasks/[task id]/_cancel`로 작업을 취소할 수 있다.
- 갑작스런 부하 등에 대한 대처를 할 수 있게 동적으로 쓰로틀링 설정값들을 변경할 수 있다. `POST _update_by_query/[task id]/_rethrottle?매개변수` 로 통해 가능하다.
- 비동기로 처리된 작업들은 .task인덱스에 남으므로 `DELETE .tasks/_doc/[task id]`로 삭제하면 좋다.

- **슬라이싱**이라는 기능이 있다.
  - 매개변수를 지정하면 검색과 업데이트를 지정한 개수로 쪼개 병렬적으로 수행한다. 즉, 성능을 극한으로 끌어올려 처리할 때 사용한다. `POST [인덱스 이름]/_update_by_query?slices=auto`
  - 기본값은 1이며, 작업을 병렬로 쪼개지 않는다는 것이다. 
  - 주 샤드 수를 넘는 값을 할당하면 성능이 급감할 수 있다.
- delete by query는 사실상 update by query와 비슷하다.

