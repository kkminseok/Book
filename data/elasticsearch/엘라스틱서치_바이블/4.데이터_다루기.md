엘라스틱 서치가 제공하는 다양한 API를 다룰 수 있는 챕터

## 1.단건 문서 API

하나의 문서에 대한 조작을 진행할 수 있는 API이다.

### 색인 API

- 앞 장에서 이야기했던 API들이다. POST, PUT, id값을 추가하거나 추가하지 않아서 문서를 만들 수 있다.
- 성능 향상을 위해 라우팅 기능을 추가하여 색인을 진행할 수도 있다.
- **refresh**라는 매개변수를 통해 색인한 직후 검색할 수 있게도 할 수 있다.
  - 예상하겠지만 비용이 크게 들기에 많은 고려가 필요하다.
  
### 조회 API

- `GET`을 이용한 API들이다. _doc,_source로 조회가 가능하다.
  - _doc은 전체 메타데이터를 확인할 수 있지만, _source는 메타데이터 없이 본문만 조회 가능하다.
- 검색과 다르게 색인이 refresh되지 않은 상태에서도 변경된 내용을 확인할 수 있다.
- **필드 필터링**을 통해서 결과에 원하는 필드만 필터링해 포함시킬 수 있다.
  - _soruce_includes: 지정한 필드만을 결과에 포함한다.
  - _source_excludes: 지정한 필드외에 값을 결과에 포함한다.
  - `GET myindex2/_doc/1?_source_includes=t*` 이런식으로 와일드카드로 표현도 가능하다.
- 색인했을 때 라우팅을 지정하였다면 조회시에도 라우팅키값으로 조회해야한다.

### 업데이트 API

- PUT으로 색인이 생성가능하므로 일반적인 API와 달리 POST + _update라는 구문으로 업데이트할 수 있다.
- **업데이트 방식은 기존 문서를 수정하는게 아닌, 기존 문서와 업데이트 될 내용을 합쳐서 새 문서로 만들어 색인한다.**
- doc에 내용을 **직접 입력**하여 업데이트 할 수도 있다.
- **detect_noop**이란 업데이트 내용이 기존 문서 내용을 실질적으로 변경하는 여부를 확인하여 만약 기존 문서와 동일하면 noop 요청이라고 판단하고 쓰기 작업을 진행하지 않는다. 기본적으로 활성화되어 있다.
- 업데이트 API는 기존 문서를 읽는 동작방식을 지닌다 했는데, 만약 기존 문서가 없다면 **doc_as_upsert**라는 옵션을 통해서 새로 문서를 추가할 지 여부를 추가할 수 있다.
- script를 통한 업데이트도 가능한데, 엘라스틱서치의 자체 스크립트 언어인 painless를 사용한다. 더 궁금하면 찾아보면 될 듯 하다.
- 색인 API와 마찬가지로 라우팅과 refresh 옵션을 지정할 수 있다.


### 삭제 API

지정한 문서 하나를 삭제한다. `DELETE` 메서드를 사용하면 되고, 큰 특이사항은 없다. 조심해야할건 `DELETE [인덱스 이름]` 으로 개발자가 실수하여 doc를 지정하지 않으면 인덱스 전체가 삭제될 수 있다는 것이다.

## 2. 복수 문서 API

단건 API를 여러번 날리는건 네트워크 비용이 들 수 밖에 없다.

한 번의 요청으로 여러 요구사항을 처리할 수 있으면 좋다. 그래서 복수 문서에 대한 제어가 필요하면 여기서 설명되어지는 API를 사용하면 될 것 같다.

### bulk API

여러 색인, 업데이트 삭제 작업을 한 번의 요청에 담아서 보내는 API다.

- NDJSON이라는 여러 줄의 JSON을 줄바꿈 문자로 구분하여 요청을 보내는 형태를 사용한다. 따라서 헤더도 application/json 대신 application/x-ndjson을 사용하고, 가장 마지막 줄 줄바꿈 문자 \n으로 끝나야 한다.

```json
POST _bulk
{"index": {"_index": "bulk_test","id":"1"}}
...
```

이런식으로 사용이 가능하다.

- 색인 요청을 할 때 index, create 요청이 사용되는데, index는 기존에 동일한 _id가 존재하면 덮어 씌우고 create는 새 문서를 생성하는 것만 허용한다.
- `POST [인덱스 이름]/_bulk` 이런식으로 인덱스 이름을 넣으면 요청의 기본 대상이 해당 인덱스로 지정된다.
- **여러 요청에 대한 작업 순서는 랜덤이다.** 그 이유는 노드가 요청을 수신하면 각 요청의 내용을 보고 적절한 주 샤드로 요청을 넘기는 작업이 있기 때문이다. 단, 이러한 동작으로 인해 동일한 문서에 대한 bulk작업은 순서 보장이 된다.


### multi get API


_id를 여럿 지정하여 해당 문서를 한 번에 조회하는 API

- `GET _mget` 요청을 통해 수행할 수 있으며 본문에 `docs` 키에 조회할 문서들을 담으면 된다. 이러면 응답은 여러 문서에 대한 내용이 한 번에 오게 된다.

### update by query, delete by query

검색 쿼리를 통해 주어진 조건을 만족하는 문서를 찾은 뒤 그 문서를 대상으로 업데이트나 삭제 작업을 실시하는 API다.

- `POST [인덱스 이름]/_updated_by_query` 
- 업데이트 도중 다른 동작으로 인해 문서에 변화가 생길 수 있는데, 특정 옵션을 통해 이런 충돌에 대한 대응을 할 수 있다. 기본값은 abort로 충돌 발견 시 작업을 중단한다. 다른 상태로는 proceed가 있다. **참고로 충돌이 발생하고 작업이 멈추면 롤백되지 않아서 중단 전까지 변경된 내용은 그대로 남아 있는다.**
- **스로틀링 기능**이 있는데, 이 기능을 통해 작업의 속도를 조정하고 클러스터 부하와 서비스 영향을 최소화할 수 있다. 스크롤링 사이즈를 지정하면 그 사이즈만큼 작업을 순차로 진행한다. 예로 들어서 스크롤링 사이즈를 1000으로 하였다면 1~1000개의 문서를 가져온 뒤 업데이트를 수행하고 1001~2000번의 문서를 가져와서 작업을 수행한다.
- scroll_size를 통해 한 번에 가져올 문서 크기를 정할 수 있고, scroll=1m와 같은 값을 통해 가져온 문서에 대한 데이터들을 얼머나 보존할 지 정할 수 있다. 모든 작업이 종료될 때까지 필요한 시간을 지정하는 것이 아닌, 한 배치 작업에 필요한 시간을 지정하면 된다.
- 스로틀링을 실제 적용하기 위해서는 `requests_per_second` 설정을 이용한다. 이 값은 이름 그대로 평균적으로 초당 몇 개까지의 작업을 수행할 것인지를 지정한다. 예를 들어서 `scroll_size`가 2000이고, `requests_per_second`값이 500이라면 다음과 같이 동작할 수 있다.
  - 만약 2000개의 문서를 업데이트하는데 0.5초를 사용했다면 3.5초를 기다려서 초당 500개의 문서를 처리한 것처럼 대기시간을 조절하여 4초마다 스크롤 하나의 분량을 처리할 수 있게 한다(requests_per_second가 초당 500개의 분량을 해결할 수 있도록 설정하였으므로)
- `wait_for_completion` 매개변수를 통해 update by query에 대한 비동기 요청이 가능하다.
  - update by query에 대한 동작은 task의 형태로 동작하며 해당 값에 부여된id를 통하여 진행상태 조회가 가능하다. 비동기로 동작시키게끔 하면 .task라는 내부 인덱스에 문서로 저장되고 여기서 진행 상황을 확인할 수 있다.(`GET _task/[task id]`)
    - `POST _tasks/[task id]/_cancel`로 작업을 취소할 수 있다.
- 갑작스런 부하 등에 대한 대처를 할 수 있게 동적으로 쓰로틀링 설정값들을 변경할 수 있다. `POST _update_by_query/[task id]/_rethrottle?매개변수` 로 통해 가능하다.
- 비동기로 처리된 작업들은 .task인덱스에 남으므로 `DELETE .tasks/_doc/[task id]`로 삭제하면 좋다.

- **슬라이싱**이라는 기능이 있다.
  - 매개변수를 지정하면 검색과 업데이트를 지정한 개수로 쪼개 병렬적으로 수행한다. 즉, 성능을 극한으로 끌어올려 처리할 때 사용한다. `POST [인덱스 이름]/_update_by_query?slices=auto`
  - 기본값은 1이며, 작업을 병렬로 쪼개지 않는다는 것이다. 
  - 주 샤드 수를 넘는 값을 할당하면 성능이 급감할 수 있다.
- delete by query는 사실상 update by query와 비슷하다.

## 3. 검색 API

엘라스틱서치의 기본이자 핵심은 검색 엔진

### 검색 대상 지정

```text
GET [인덱스 이름]/_search
POST [인덱스 이름]/_search
GET _search
POST _search
```

### 쿼리 DSL 검색, 쿼리 문자열 검색

요청 본문에 엘라스틱서치 전용 쿼리 DSL을 기술하여 검색하는 방법과 요청 주소줄에 q라는 매개변수를 넣고 검색하는 방법이 있는데, **두 가지 방법은 혼용할 수 없다**

- 쿼리 DSL검색

```text
GET my_index/_search
{
  "query": {
    "match": {
      "title": "minseok"
    }
  }
}
```

- 쿼리 문자열 검색
```text
GET my_index/_search?q=title:minseok
```
위 처럼 지정한 필드로 검색하는 것 외에 존재 여부, 와일드카드, AND 등 연산을 추가할 수 있다.

### match_all 쿼리

모든 문서를 매치하는 쿼리. 

```text
GET my_index/_search
{
  "query": {
    "match_all": {}
  }
}
```

### match 쿼리

지정한 필드의 내용이 질의어와 매치되는 문서를 찾는 쿼리.

- 예시는 위의 쿼리 DSL예시에서 본대로
- 애널라이저에 따른 쿼리 결과 값이 달라진다. 

### term, terms 쿼리

term쿼리는 지정한 필드의 값이 질의어와 정확히 일치하는 문서를 찾는 쿼리.
terms쿼리는 term쿼리를 이용해 질의어를 여러 개 지정할 수 있고, 하나 이상의 질의어가 일치하면 검색 결과에 포함됨.

```text
GET my_index/_search
{
  "query": {
    "term": {
      "fileName" :{
        "value": "hello"
      }
    }
  }
}
// or..

terms: {
    "fileName": ["hello","world"]
}
```

### range 쿼리

지정한 필드의 값이 특정 범위 내에 있는 문서를 찾음.
부하가 드는 작업이므로 주의가 필요함.

```text
GET my_index/_search
{
  "query":{
    "range": {
      "fileName": {
        "gte": 100,
        "lt": 200
      }
    }
  }
}
```

### prefix 쿼리

지정한 질의어로 시작하는 문서를 찾는 쿼리
이 또한 무거운 쿼리이다.

### exists 쿼리

지정한 **필드를 포함한 문서**를 검색

### bool 쿼리

여러 쿼리를 조합하여 검색하는 쿼리. 

여러 쿼리를 실행하는 순서가 있을텐데, 이는 엘라스틱 서치 내부에서 비용한 효과를 추정해서 알아서 최적화하여 검색하게 된다. 따라서 정해진 순서는 알 수 없다. 

### constant_score 쿼리

기본적으로 Elasticsearch는 **검색어와 문서의 관련성(relevance)**을 계산할 때 BM25(TF-IDF 기반)을 사용하여 스코어를 부여한다.

하지만, 경우에 따라 점수 계산이 불필요하거나 정렬, 필터 용도로만 사용할 때는 모든 문서에 동일한 점수를 주는 게 효율적이다. 이럴 때 constant_score를 쓰면:

- 점수 계산을 생략해 성능을 높일 수 있고,
- 필터로 동작하므로 쿼리 구조를 명확히 할 수 있다.

### 그 외 주요 매개변수

쿼리 종류에 상관없이 검색 API에 공통적으로 적용할 수 있는 주요 매개변수 몇 가지가 존재하다.

- 라우팅
  - 검색 API도 색인 API나 조회 API와 마찬가지로 라우팅을 제대로 지정해주는게 좋다. `_search?routing=[라우팅]` 으로 적용 가능
- explain
  - 검색을 수행하는 동안 쿼리의 각 하위 부분에서 점수가 어떻게 계산되었는지 설명해줌. 디버깅용 `_search?explain=true`로 적용가능.
    - bool쿼리와 조합해서 사용시 어느 쿼리가 오래 걸리는지 확인이 대강 가능.
- search_type
  - 유사도 점수를 계산할 때 각 샤드 레벨에서 계산을 끝낼지 여부를 선택 가능.
  - query_then_fetch와 dfs_query_then_fetch 두 가지 옵션을 선택할 수 있다.
    - query_then_fetch: 기본 설정. 각 샤드 레벨에서 유사도 점수 계산을 끝냄
    - dfs_query_then_fetch: 모든 샤드로부터 정보를 모아 유사도 점수를 글로벌하게 계산. 점수 계산의 정확도는 올라가지마 성능을 저하될 수 있다.

### 검색 결과 정렬

요청 본문에 `sort`를 지정하면 검색 결과를 정렬할 수 있다.
- 여러 대상을 지정할 수 있다.
- 이름만 명시하면 내림차순 정렬을 한다.
- fielddata를 true로 지정하지 않은 text자료형은 정렬이 불가능하다.
- 필드 이름 외에도 **_score**나 **_doc**를 지정할 수 있다.
- 정렬 수행 중에는 필드의 값이 메모리에 올라간다. 때문에 정렬을 지속적으로 사용한다면 메모리를 적게 사용하는 integer, short, float 등의 타입으로 설계하는것도 운영에서 참고할만한 점이다.
- 

``text
{
    "query" : {
        // ...
    },
    "sort": [
        { "field1": { "order": "desc" }},
        { "field2": { "order": "asc"}},
        "field3"
    ]
}
```

### 페이지네이션

- 기본적으로 사용할 수 있는 `from`, `size`
    - from값이 커질수록 매우 무거운 검색을 수행
    - 인덱스 상태가 일관되지 않아서 중간에 새로운 문서가 색인되거나 하는 등 상태 변화가 생기면 문제가 생길 수 있다.
    - **추천되는 방식은 아님.**
```text    
GET my_index/_search
{
    // 11번째부터 15번째까지 5개의 문서 반환
    "from": 10,
    "size", 5,
    "query": {
        // ...
    }
}
```

- 검색에 매칭되는 전체 문서를 모두 확인할 때 사용하는 `scroll`
  - 위와 달리 최초 검색 시의 문맥이 유지됨. 중복이나 누락이 발생하지 않는다. 매개변수로 검색 문맥을 유지할 시간을 지정할 수 있음.
  - 검색결과로 **_scroll_id**를 받을 수 있는데 두 번째 검색부터는 이 값을 사용해서 검색을 수행하면 된다. 더 이상 문서가 반환되지 앟을 때까지, 즉 빈 **hits**가 들아올 때까지 검색한다.
  - 검색을 수행할 때마다 검색 문맥이 연장된다. 
  - 검색 문맥은 유지 시간이 종료되면 자동으로 삭제되지만 명시적으로 삭제도 가능하다. `DELETE _search/scroll + 본문 scroll_id`
  - 본래 지속적으로 해당 기능을 호출하라고 만들어진 기능은 아님. 주로 대량의 데이터를 다른 스토리지로 이전하거나 덤프용으로 만들어졌다. 지속적으로 호출하기 위한 페이지네이션은 **search_after**를 사용하는게 좋다.

```text
GET my_index/_search?scroll=1m
{
    "size": 1000,
    "query" :[
        //...
    ]
}
```

- 성능 부담은 상대적으로 낮춘 채 페이지네이션을 할 수 있는 `search_after`
    - sort를 지정해야함. 동일한 정렬 값이 등장할 수 없도록 최소한 1개 이상의 동점 제거용 필드도 지정해야함. _id를 직접 사용하는건 메모리 부담이 생길 수 있기에 따로 동일한 값을 필드에 저장하여 사용하는 건 괜찮다.
    - 처음 검색해서 나온 sort 기준값으로 다음 검색시에 추가한다.
    - 인덱스 상태가 변하는 도중에 검색하면 페이지네이션 과정에서 누락되는 문서가 발생할 수 있다. 떄문에 **point in time API**와 함께 조합하여 사용하면 검색 대상의 상태를 고정하여 검색할 할 수 있다. `POST my_index/_pit?keep_alive=1m`와 같은 문법을 사용하면 id값을 받고 이를 가지고 페이지네이션을 구현할 수 있다. `DELETE _pit + 본문 pit id`로 명시적으로 삭제가 가능하다.
      - pit id로 검색 대상을 지정하기에 별도의 인덱스를 지정할 필요가 없고, 정렬 기준 필드를 하나라도 지정 했다면 동점 제거용 필드도 별도로 지정할 필요가 없다. 

```text
// 첫 검색
GET my_index/_search
{
    "size": 20,
    "query": {
        //..
    },
    "sort": [
        {
            "order_date": "desc"
        }, 
        ...
    ]
}

//결과
{
    "hits": {
        // ...
        "hits": [
            // ...
            {
                // ...
                "sort": [
                    16743335900000,
                    "591924"
                ]
            }
        ]
    }
}

// 두번째 검색
GET my_index/_search
{
    "size": 20,
    "query": {
        // ...
    },
    "search_after": [16743335900000, "591924"],
    "sort": [
        {
            "order_date": "desc"
        }, 
        ...
    ]
}
```


## 총평

어렵다. 직접 여러가지 테스트하면서 이해하는게 빠를 것 같다. 참고하여 회사에서 여러 조건으로 검색, 테스트 해봐야겠다.