분산 시스템이 야기할 수 있는 문제, 해결방법 등을 다룬 챕터

## 기본적인 분산 데이터 구조

책에서 시계, ID와 사전 기록 로그 라고 되어있는데 도통 이게 어떤걸 의미하는지 몰라서 찾아봤다. 책에서는 잘 안 나와있다;

3개 모두 **여러 노드 간에 발생하는 이벤트 순서와 일관성 관리**에 초점이 맞춰져있다.

- 시게: 이벤트의 시간적인 순서를 결정함. 분산 시스템에서는 독립적인 시계를 가지고 있어서 동기화가 필요함.
    - 물리 시계(Physical Clocks): 컴퓨터의 시계. 실제 시간을 측정함. 각 노드의 물리 시계가 정확히 일치하기 어려워 이벤트 순서를 결정하는 데 한계가 존재.
    - 논리 시계(Logical Clocks): 물리적인 시간보다는 **이벤트 간의 인과간계**를 추적하는데 중점.
        - 람포트 타임스탬프(Lamport Timestamps): 이벤트에 숫자를 할당하여, 이벤트가 발생할 때마다 숫자를 증가시킴. "A가 B보다 먼저 발생했다면, A의 타임 스탬프는 B의 타임스탬프보다 작다"는 보장하지만, 그렇다해서 A가 B보다 먼저 발생했다고는 단정할 수 없다는 한계가 존재.
    - 벡터 시계(Vector Clocks): 각 노드에 벡터(배열)을 할당하여, 노드별 이벤트 카운터를 기록. **이벤트 간의 부분 순서를 정확히 파악**하고, 어떤 이벤트가 동시에 발생했는지도 알 수 있음.
- ID(Identifiers): 각 이벤트, 데이터 객체 또는 노드를 고유하게 식별하는 데 사용.
    - 고유성: ID는 전역적으로 유일해야함.
- 사전 기록 로그(Pre-commit log): 어떤 작업이 실제로 실행되기 전에 그 내용을 미리 기록하는 로그
    - 내구성 보장: 로그로 기록되어 있기에 데이터를 복구할 수 있음.
    - 원자성 보장: 분산 트랜잭션에서 모든 참여 노드가 작업을 완료하거나 실패했을 때 모든 노드가 롤백하여 원래 상태로 돌아가는 것을 보장.
    - 일관성 유지: 여러 복제본 간에 데이터 일관성 유지를 위해, 변경 사항을 로그에 기록하고 이를 다른 복제본에 전파하는 데 사용

### 직렬화

직렬화는 객체 인스턴스 상태를 바이트 스트림으로 변환하는 과정인데, 보통 JSON을 직렬화 형식으로 사용한다. 그 이유는 단순하고, 사람이 읽을 수 있고, 라이브러리 지원이 후하기 떄문이다.

그런데 직렬화된 객체의 크기와 같은 요인들이 성능 저하를 초래할 수 있어서, 대용량 고성능 애플리케이션에서는 바이너리 인코딩이 적합할 수 있다. 프로토콜 버퍼나 에이브로 등 다양한 옵션이 존재한다.

### 데이터 분할과 복제

- 데이터 샤딩: 대규모 데이터셋을 더 작고 관리하기 쉬운 하위 집합, **파티션**으로 나눔.
    - 수평 분할: 특정 키나 중요한 필드의 해시 값을 기반으로 데이터 분할
    - 수직 분할: 데이터를 열 단위로 나눔.

### CAP 정리

분산 시스템에서 잘 알려진 이론
- C(Consistency): 일관성
    - 모든 클라이언트가 어떤 노드를 연결하든 **동일한 최신 데이터를 볼 수 있어야 함**
- A(Availability): 가용성
    - 모든 요청에 대해 **항상 응답할 수 있어야 함**
- P(Partition Tolerance): 분할 내성
    - 네트워크 단절이 발생하더라도 시스템이 **계속해서 동작할 수 있어야 함**

세 가지 속성 중에서 **오직 두 가지만을 완벽하게 만족할 수 있다**는 것을 의미.

- CP 시스템: 네트워크 분할 상황에서 **일관성을 유지하는 것을 우선**시함. 데이터 동기화가 불가능 하면, 데이터 불일치를 피하기 위해 오류나 응답 거부를 함. -> 가용성 희생
    - ex) 은행 시스템, MongoDB, ...
- AP 시스템: 네트워크 분할 상황에서 **가용성을 유지** 최신 데이터가 아니여도 응답을 제공하는것에 초점
    - ex) 소셜 미디어의 '좋아요', '댓글' 기능, Dynamo DB, Cassandra, ...
- CA 시스템: **네트워크 분할을 허용하지 않는 시스템**에서만 일관성과 가용성을 동시에 보장, 사실상 **단일 노드 시스템**. 현실적으로 분산 시스템에서는 만족하기 불가능


## 합의 프로토콜

합의 프로토콜이란 분산 시스템 내의 여러 프로세스(노드)들이 하나의 값(결정)에 대해 **동일한 의견**을 모으는 과정. 네트워크 지연, 노드 장애 등의 불안정한 환경에서도 정확하고 신뢰할 수 있는 합의에 도달해야한다.

두 가지 목표를 지니고 있는데, 첫 번째는 **안정성**이다. 잘못된 값이 결정되지 않아야 하며, 두 개의 다른 값이 동시에 결정되지 않아야 한다. 두 번째는 **생존성**이다. 시스템이 장애 상황에서도 궁극적으로 합의에 도달해야한다. 즉, 리더가 죽어도 새로운 리더가 선출되어 서비스가 지속되어야 한다.

### 팩소스(Paxos)

1990년대 고안된 **최초의 분산 합의 프로토콜 중 하나**이다. 매우 복잡한 개념을 가지고 있어서 잘 사용되지는 않는다.

작동방식은 다음과 같다. 

일단, Paxos는 **Proposer, Acceptor, Learner** 세 가지 역학을 하는 노드들이 상호작용하며 합의에 도달한다.

1. Prepare 단계: Propose는 제안 번호를 할당 받아 Acceptor들에게 제안을 준비한다는 메시지를 보낸다.
2. Promise 단계: Acceptor는 자신이 기억하는 가장 큰 제안 번호보다 Proposer의 제안 번호가 크면, 더 이상 작은 제안 번호를 가진 제안은 수락하지 않겠다는 Promise와 함께 자신이 이전에 수락했던 값을 Proposer에게 보낸다.
3. Accept 단계: Proposer는 다수의 Acceptor로부터 Promise를 받으면, 수락된 값 중 가장 큰 제안 번호를 가진 값을 선택하거나 새로운 값을 제안하여 Acceptor들에게 수락 요청(Accept)을 보낸다.
4. Accepted 단계: Acceptor는 제안을 수락하면, 이 사실을 Learner에게 알린다.

이론적으로 **가장 안전하고 강력한 합의 보장**을 제공하지만, 알고리즘의 복잡성, 디버깅의 어려움, 이해의 어려움, 오버헤드 존재 등의 문제도 가지고 있다.

### 래프트(Raft)

Paxos의 복잡성을 개선하여 **이해하기 쉬운 합의 알고리즘**을 목표로 한다.

작동 방식은 다음과 같다.

Raft는 시스템 내의 노드들이 다음 세 가지 상태 중 하나를 갖는다.

- Follower: Leader로 부터 요처을 받거나 Candidate의 투표 요청에 응답한다.
- Candidate: 새로운 Leader를 선출하기 위해 투표를 요청하는 상태이다.
- Leader: 모든 클라이언트 요청을 처리하고, 로그 복제를 관리하며, Follwer들에게 주기적으로 하트비트를 보내 자신의 존재를 알린다.

핵심 메커니즘은 다음과 같다,

1. 리더 선출(Leader Election)
    - Follower는 일정 시간 동안 Leader로부터 하트비트를 받지 못하면 Candidate로 전환하고, 자신에게 투표한 후 다른 노드들에게 투표를 요청
    - 과반수 이상의 투표를 받으면 Candidate는 Leader로 승격
    - 새로운 Leader는 즉시 모든 Follower에게 하트비트를 보내 자신의 선출을 알림.
2. 로그 복제(Log Replication)
    - 클라이언트 요청은 Leader에게 전달
    - Leader는 커맨드를 자신의 **로그 엔트리**에 추가하고, 이 엔트리를 Follower들에게 복제
    - Follower들이 성공적으로 로그 엔트리를 복제하고 응답하면, Leader는 해당 엔트리를 **커밋**하고 클라이언트에게 응답
    - 커밋된 로그 엔트리는 모든 노드에 적용되어 분산 시스템의 상태를 일관되게 만듦.
3. 안정성(Safety)
    - Leader Complete Log, Commit Index 개념을 통해 항상 최신 커밋된 로그를 포함하는 Leader를 선출하고, Leader만이 로그를 추가할 수 있도록 하여 데이터 일관성을 보장

이해하기 쉽고, 강력한 일관성과 구현 용이성 등을 지니고 있지만, Leader에게 로직들이 의존되어 있어서 병목이 존재할 수 있고 네트워크가 단절되어 과반수 노드가 연결되지 않으면 정상적으로 동작하지 않을 수 있다는 단점이 있다.

## 분산 시스템 예제

### 분산 데이터 베이스: 카산드라

카산드라는 AP시스템이다. 즉, 네트워크가 단절되더라도 높은 가용성을 보장한다. 일시적으로 데이터가 불일치할 수 있지만 **최종 일관성** 메커니즘을 통하여 해결할 수 있다.

카산드라는 마스터리스(Masterless) 아키텍처라서 모든 노드가 동일한 역할을 수행한다. 즉, 단일 실패 지점(SPF)가 없어서 가용성을 보장한다.

카산드라의 가장 중요한 특징 중 하나는 **일관성 수준을 읽기 및 쓰기 작업별로 조절**이 가능하다. 

- 쓰기 일관성 수준(Write Consistency Levels)
    - One: 하나의 복제몬만 쓰기 작업을 승인하면 성공으로 간주. 빠르지만 데이터 손실 가능성 존재
    - QUORUM: (복제본 개수 /2 +1)이 쓰기 작업을 승인해야 성공으로 간주됨. 높은 가용성 유지하면서 어느정도의 일관성 확보
    - ALL: 모든 복제본이 쓰기 작업을 승인해야 성공으로 간주. 가용성이 저하될 수 있지만, 일관성은 가장 크다.

- 읽기 일관성 수준(Read Consistency Levels)
    - ONE: 하나의 복제본에서 데이터를 읽어옴. 빠르지만, 옛날 데이터를 읽을 수도 있다.
    - QUORUM: 대부분의 복제본에서 데이터를 읽어와 최신 타임스탬프를 가진 데이터를 반환
    - ALL: 모든 복제본에서 데이터를 읽어와 최신 데이터를 보장. 노드 장애 시 읽기 실패 가능성 존재

- 최종 일관성 (Eventual Consistency)
    - Hinted Handoff: 특정 복제본이 오프라인일 때, 코디네이터 노드는 해당 복제본을 위해 데이터를 잠시 보관(힌트)하고 있다가, 복제본이 다시 온라인 상태가 되면 데이터를 전달
    - Read Repair: 읽기 요청 시 코디네이터는 여러 복제본에서 데이터를 읽어와 불일치를 발견하면 백그라운드에서 오래된 데이터를 가진 복제본을 업데이트함.
    - Anti-Entropy (Merkle Trees): 노드들은 주기적으로 자신의 데이터 해시를 비교하여 불일치를 찾아내고 동기화합니다. 이는 큰 스케일에서 발생하는 데이터 불일치를 효율적으로 해결하는 데 사용됨.

    
### 인메모리 데이터 그리드: 인피니스팬

책에서 너무 설명을 못해서.. 따로 찾아본 결과를 정리한다.

인피니스팬(Infinispan)은 다양한 캐시 모드를 지원한다. 로컬 캐시, 분산 캐시, 복제 캐시, 무효화 캐시 등 다양한 모드를 지원해서 요구사항에 맞게 성능과 일관성을 선택할 수 있다.

고급 기능으로는 **영속성, 트랜잭션, 쿼리 및 인덱싱, 고가용성, 확장성, 데이터 이벤트**등을 제공하고 있다.

보통 인피니스팬은 고성능 분산캐시가 필요한곳에서 사용된다. 다양한 고급기능을 제공하고 있어서 대규모 분산 시스템에서 사용되지만, 복잡성과 JVM의존성, 커뮤니티 및 생태계의 규모가 작다는 점을 단점으로 가지고 있다.

### 이벤트 스트리임: 카프카

이 부분도 따로 대강 찾아봤다.

카프카의 아키텍처와 핵심 메커니즘은 다음과 같다.

1. 분산 로그(Distributed Log) 아키텍처
    - 토픽과 파티션: 데이터를 **토픽**이라는 논리적 범주로 나누고, 각 토픽은 여러개의 **파티션**으로 구성됨. 각 파티션은 순서가 보장되는 불변의 커밋 로그이다.
    - 리더와 팔로워: 각 파티션에는 하나의 **리더**와 여러 개의 **팔로워**가 존재
        - 리더: 모든 쓰기 및 읽기 요청을 처리
        - 팔로워: 리더의 로그를 복제하여 동기화 상태를 유지
    - 분산 및 복제: 파티션은 클러스터 내의 여러 브로커에 분산되어 저장되며, 각 파티션의 복제본은 서로 다른 브로커 위치한다. 때문에 브로커가 다운되어도 데이터 손실 없이 서비스 연속성을 보장함.
2. 고가용성 보장
    - 리더 선출: 요즘은 앞에서 설명한 Raft 합의 프로토콜로 진행(KRaft)
    - 복제: 데이터는 여러 브로커에 복제되므로, 일부 브로커가 다운되더라도 다른 복제본을 통해 데이터를 계속 읽고 쓸 수 있음.
3. 유일한 일관성 제공

Producer, Consumer는 각각 acks설정과 offset관리를 통해 원하는 일관성 수준을 조절할 수 있도록 함.

- 프로듀서의 일관성(쓰기 일관성)
메시지를 보낼 때 `acks`설정을 통해 메시지가 얼마나 많은 브로커에 기록되어야 성공으로 간주할지 지정할 수 있다.
    - `acks=0`: 브로커의 응답을 기다리지 않는다. 가장 빠르지만 메시지 손실 가능성이 존재
    - `acks=1`: 리더 브로커가 메시지를 받았음을 확인하면 성공으로 간주. 리더가 다운되면 메시지 손실 가능성 존재
    - `acks=all(또는 -1)`: 리더 브로커와 모든 ISR(In-Sync-Replicas)에 있는 팔로워들이 메시지를 성공적으로 복제했음을 확인해야 성공으로 간주. 높은 일관성, 팔로워 중 하나라도 문제가 생기면 쓰지 지연이 발생하거나 실패 가능성 존재

- 컨슈머의 일관성(읽기 일관성)
    - 오프셋 커밋(Offset Commit): 컨슈머는 메시지를 처리한 후 자신이 어디까지 읽었는지 오프셋을 커밋함. 
    - 최종 일관성: 리더 브로커로부터 데이터를 읽어오며, 프로듀서의 `acks`설정에 따라 읽는 시점에 최신 데이터가 아닐 수 있음.















## 총평

용어 설명도 없고 용어도 너무 난잡하게 쓰이는 감이 없지 않아 있다. 

"사전 기록 로그를 소개하겠습니다. 이 기술은 데이터베이스 시스템에서 원자성과 지속성을 보장하기 위해 널리 사용됩니다." 라고 하고 설명이 끝나는데, 내용이 불친절하다. 막연히 봤을 때 원자성과 지속성을 왜 로그를 통해서 보장되어야하는가? 에 대한 설명이 없기 때문이다. 그래서 결국 따로 찾아보고 내용을 추가할 수 밖에 없었다.

거의 LLM과 함께 공부했다.
